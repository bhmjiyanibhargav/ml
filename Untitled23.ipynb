{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16b10378",
   "metadata": {},
   "source": [
    "# question 01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213e84c3",
   "metadata": {},
   "source": [
    "Polynomial functions and kernel functions are both used in machine learning algorithms to transform data into a higher-dimensional feature space. This transformation is often done to make the data more amenable to classification or regression tasks, particularly when the data is not linearly separable in its original form.\n",
    "\n",
    "Here's the relationship between polynomial functions and kernel functions:\n",
    "\n",
    "1. **Polynomial Functions**:\n",
    "   - A polynomial function is a mathematical function that involves powers of a variable raised to non-negative integer exponents.\n",
    "   - In machine learning, a polynomial feature transformation involves creating new features by taking all combinations of the original features raised to non-negative integer powers.\n",
    "   - For example, if you have a feature \\(x\\), a second-degree polynomial transformation would include features like \\(x^2\\), \\(x^3\\), and so on.\n",
    "   - Polynomial features can be used in linear models to allow them to capture non-linear relationships in the data.\n",
    "\n",
    "2. **Kernel Functions**:\n",
    "   - A kernel function, in the context of machine learning, is a function that computes a dot product in some (possibly infinite-dimensional) feature space.\n",
    "   - It allows us to implicitly represent data in a higher-dimensional space without explicitly computing the coordinates of the data points in that space.\n",
    "   - Commonly used kernel functions include polynomial kernels, Gaussian (RBF) kernels, and more.\n",
    "   - A polynomial kernel is a specific type of kernel function that computes the dot product as if the data had been transformed using a polynomial feature transformation.\n",
    "\n",
    "**Relationship**:\n",
    "\n",
    "The relationship between polynomial functions and polynomial kernels lies in the fact that a polynomial kernel is essentially performing a polynomial feature transformation, but in a more computationally efficient manner.\n",
    "\n",
    "For example, if you use a second-degree polynomial kernel, it's equivalent to applying a second-degree polynomial feature transformation to the data. However, instead of explicitly calculating the transformed features, the kernel computes the dot product in the higher-dimensional space directly.\n",
    "\n",
    "This is particularly useful when working with high-dimensional or even infinite-dimensional feature spaces, where explicitly computing the transformed features would be impractical or impossible.\n",
    "\n",
    "In summary, while polynomial functions are a specific type of feature transformation, polynomial kernels are a way to achieve a similar effect, but in a more efficient and often more powerful manner, especially in the context of SVMs and other kernel-based methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf78220",
   "metadata": {},
   "source": [
    "# question 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1ea5bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of SVM with Polynomial Kernel: 100.00%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset (for demonstration purposes)\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize an SVM classifier with a polynomial kernel\n",
    "svm_classifier = SVC(kernel='poly', degree=3, C=1.0, random_state=42)\n",
    "\n",
    "# Train the classifier on the training set\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict labels for the testing set\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy of SVM with Polynomial Kernel: {accuracy*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478eb79d",
   "metadata": {},
   "source": [
    "# question 03"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5948fd5c",
   "metadata": {},
   "source": [
    "In Support Vector Regression (SVR), the parameter \\( \\epsilon \\) is a part of the epsilon-insensitive loss function. It defines the margin of tolerance where no penalty is given to errors. Specifically, SVR tries to fit as many data points as possible within this margin.\n",
    "\n",
    "Increasing the value of \\( \\epsilon \\) in SVR will generally lead to an increase in the number of support vectors. This is because a larger \\( \\epsilon \\) allows for a wider margin of tolerance for errors, meaning that more data points can be within the margin without incurring a penalty.\n",
    "\n",
    "Here's the relationship:\n",
    "\n",
    "1. **Smaller \\( \\epsilon \\)**:\n",
    "   - A smaller \\( \\epsilon \\) means a narrower margin of tolerance for errors.\n",
    "   - SVR will try to fit the data points more precisely, potentially leading to a smaller number of support vectors. It tries to minimize the deviation of each data point from the regression line.\n",
    "\n",
    "2. **Larger \\( \\epsilon \\)**:\n",
    "   - A larger \\( \\epsilon \\) allows for a wider margin of tolerance for errors.\n",
    "   - SVR will be more lenient towards allowing data points to fall within the margin without incurring a penalty. This can lead to a larger number of support vectors.\n",
    "\n",
    "In summary, increasing the value of \\( \\epsilon \\) in SVR tends to result in a larger number of support vectors because it allows for a wider margin of tolerance for errors, accommodating more data points within the margin.\n",
    "\n",
    "It's important to note that the choice of \\( \\epsilon \\) should be based on the specific characteristics of the data and the problem at hand. A larger \\( \\epsilon \\) may be appropriate when the data has a higher level of noise or when you want a more flexible model, while a smaller \\( \\epsilon \\) may be preferred for a more precise fit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88cc615",
   "metadata": {},
   "source": [
    "# question 04"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4f8d4fa9",
   "metadata": {},
   "source": [
    "Support Vector Regression (SVR) is sensitive to several hyperparameters that can significantly impact its performance. Here's an explanation of how each parameter works and how it affects SVR:\n",
    "\n",
    "1. **Kernel Function**:\n",
    "   - **Explanation**: The kernel function determines the type of transformation applied to the data. Common choices include linear, polynomial, and radial basis function (RBF) kernels.\n",
    "   - **Effect on Performance**:\n",
    "     - **Linear Kernel**: Suitable for linear relationships in the data. It might underperform if the relationship is non-linear.\n",
    "     - **Polynomial Kernel**: Useful for capturing polynomial relationships. Higher degree polynomials can capture more complex relationships, but can lead to overfitting.\n",
    "     - **RBF Kernel**: Effective for capturing non-linear relationships. The parameter \\( \\gamma \\) (gamma) controls the width of the kernel and influences the complexity of the model.\n",
    "   - **Example**:\n",
    "     - Increase degree for polynomial kernel if you suspect a higher degree relationship.\n",
    "     - Adjust \\( \\gamma \\) for RBF kernel to control the flexibility of the model. Higher \\( \\gamma \\) leads to more complex models.\n",
    "\n",
    "2. **C Parameter**:\n",
    "   - **Explanation**: The C parameter controls the trade-off between fitting the training data well and having a smooth decision boundary.\n",
    "   - **Effect on Performance**:\n",
    "     - Smaller \\(C\\) values lead to a softer margin, allowing more misclassifications but potentially better generalization. Larger \\(C\\) values lead to a harder margin, potentially overfitting the training data.\n",
    "   - **Example**:\n",
    "     - Increase \\(C\\) when you want to focus on fitting the training data closely (potentially at the cost of overfitting).\n",
    "     - Decrease \\(C\\) when you want a wider margin and more robustness to noise.\n",
    "\n",
    "3. **Epsilon (\\( \\epsilon \\)) Parameter**:\n",
    "   - **Explanation**: The epsilon parameter defines a margin of tolerance for errors in the training data. It's a part of the epsilon-insensitive loss function.\n",
    "   - **Effect on Performance**:\n",
    "     - Smaller \\( \\epsilon \\) values result in a narrower margin of tolerance for errors. The model tries to fit the data points more precisely, potentially leading to fewer support vectors.\n",
    "     - Larger \\( \\epsilon \\) values allow for a wider margin of tolerance for errors, potentially leading to more support vectors.\n",
    "   - **Example**:\n",
    "     - Decrease \\( \\epsilon \\) when you want to fit the data points more precisely.\n",
    "     - Increase \\( \\epsilon \\) when you want to allow more data points to be within the margin.\n",
    "\n",
    "4. **Gamma (\\( \\gamma \\)) Parameter** (For RBF Kernel):\n",
    "   - **Explanation**: The gamma parameter defines how far the influence of a single training example reaches. Low values mean far reach; high values mean close reach.\n",
    "   - **Effect on Performance**:\n",
    "     - Smaller \\( \\gamma \\) values lead to smoother decision boundaries, potentially resulting in underfitting.\n",
    "     - Larger \\( \\gamma \\) values make the decision boundary more wiggly, potentially leading to overfitting.\n",
    "   - **Example**:\n",
    "     - Increase \\( \\gamma \\) when you suspect the relationship between features and target variable is highly non-linear.\n",
    "\n",
    "Remember, the optimal values for these parameters depend on the specific dataset and problem at hand. It's important to perform hyperparameter tuning using techniques like cross-validation to find the best combination of parameter values for your particular application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676d9a49",
   "metadata": {},
   "source": [
    "# question 05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad52678c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 150\n",
      "Number of features: 4\n",
      "Number of classes: 3\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Optionally, you can print some information about the dataset\n",
    "print(f\"Number of samples: {X.shape[0]}\")\n",
    "print(f\"Number of features: {X.shape[1]}\")\n",
    "print(f\"Number of classes: {len(set(y))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2829f85a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of sets:\n",
      "X_train shape: (120, 4), y_train shape: (120,)\n",
      "X_test shape: (30, 4), y_test shape: (30,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset into a training set and a testing set (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Now, you have the following variables available:\n",
    "# X_train: Training features\n",
    "# y_train: Corresponding labels for the training set\n",
    "# X_test: Testing features\n",
    "# y_test: Corresponding labels for the testing set\n",
    "\n",
    "# Optionally, you can print the shapes of the sets to verify the split\n",
    "print(\"Shapes of sets:\")\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f019b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize a StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the training data and transform both training and testing data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Now, X_train_scaled and X_test_scaled are the scaled feature sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38761edb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(kernel=&#x27;linear&#x27;, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(kernel=&#x27;linear&#x27;, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(kernel='linear', random_state=42)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Initialize an instance of SVC\n",
    "svm_classifier = SVC(kernel='linear', random_state=42)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "svm_classifier.fit(X_train_scaled, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d5ef164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict labels for the testing data\n",
    "y_pred = svm_classifier.predict(X_test_scaled)\n",
    "\n",
    "# Now, y_pred contains the predicted labels for the testing data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01e2a4e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.67%\n",
      "Precision: 0.97\n",
      "Recall: 0.97\n",
      "F1-Score: 0.97\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Calculate precision\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "# Calculate recall\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "# Calculate F1-score\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f\"Accuracy: {accuracy*100:.2f}%\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1-Score: {f1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0dcab023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'C': 10, 'gamma': 'scale', 'kernel': 'linear'}\n",
      "Accuracy with Best Parameters: 96.67%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the hyperparameters and their potential values\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'poly', 'rbf'],\n",
    "    'gamma': ['scale', 'auto'],\n",
    "}\n",
    "\n",
    "# Initialize an SVC classifier\n",
    "svm_classifier = SVC()\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(svm_classifier, param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Perform the grid search on the training data\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Get the best estimator (classifier with the best parameters)\n",
    "best_classifier = grid_search.best_estimator_\n",
    "\n",
    "# Use the best classifier to predict labels for the testing data\n",
    "y_pred_best = best_classifier.predict(X_test_scaled)\n",
    "\n",
    "# Calculate the accuracy of the best classifier\n",
    "accuracy_best = accuracy_score(y_test, y_pred_best)\n",
    "\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"Accuracy with Best Parameters: {accuracy_best*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6965e234",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(C=10, kernel=&#x27;linear&#x27;, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(C=10, kernel=&#x27;linear&#x27;, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(C=10, kernel='linear', random_state=42)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize an SVC classifier with the best parameters\n",
    "best_classifier = SVC(C=best_params['C'], kernel=best_params['kernel'], gamma=best_params['gamma'], random_state=42)\n",
    "\n",
    "# Scale the entire dataset (if not already scaled)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train the classifier on the entire dataset\n",
    "best_classifier.fit(X_scaled, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2987877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The trained classifier has been saved to 'svm_classifier_model.joblib' for future use.\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Define the file path where you want to save the classifier\n",
    "file_path = 'svm_classifier_model.joblib'\n",
    "\n",
    "# Save the trained classifier to the file\n",
    "joblib.dump(best_classifier, file_path)\n",
    "\n",
    "print(f\"The trained classifier has been saved to '{file_path}' for future use.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27210835",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
